# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
##### Predicting Profits #####
#Calc profit per book
book_cost = 15*1.45 #each book costs $15 plus 45% overhead
book_profit = 31.95-book_cost #each book is sold for $31.95
#If sending mailings to entire list
total_list = 50000 #list of 50,000 customers to mail ad to
total_cost = 0.65*total_list #cost to mail entire list an ad
#According to case study prompt, 9.03% of mailings resulted in an order
total_profit = 0.0903*total_list*book_profit-total_cost
#If using logistic regression model to selectively send mailings
#predicted (495+138)/(2300)=27.5% would purchase a book with only 138/(495+138) actually buying the book (21.8%)
model_list = total_list*0.275 #Number of customers to send mailings to
model_cost = 0.65*model_list
model_profit = 0.218*model_list*book_profit-model_cost
summary(log.model4)
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender +  Frequency + P_Child + P_Cook + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, -which(names(train_bal) == "Choice")],   # All predictors except the target variable
train_bal$Choice,   # Target variable
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
### Adding SVM Model (with balanced data) ###
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, -which(names(train_bal) == "Choice")],   # All predictors except the target variable
train_bal$Choice,   # Target variable
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, -which(names(train_bal) == "Choice")],   # All predictors except the target variable
train_bal$Choice,   # Target variable
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
### Adding SVM Model (with balanced data) ###
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, -which(names(train_bal) == "Choice")],   # All predictors except the target variable
train_bal$Choice,   # Target variable
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, -which(names(train_bal) == "Amount_purchased", "Frequency", "P_Child", "P_Cook", "P_DIY", "P_Art")],   # All predictors except the target variable
train_bal$Choice,   # Target variable
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# List of predictor variables
predictor_vars <- c("Amount_purchased", "Frequency", "P_Child", "P_Youth", "P_Cook", "P_DIY", "P_Art")
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, predictor_vars],   # All predictor variables
train_bal$Choice,   # Target variable 'Choice'
sizes = c(1:8),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
### Adding SVM Model (with balanced data) ###
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# List of predictor variables
predictor_vars <- c("Amount_purchased", "Frequency", "P_Child", "P_Youth", "P_Cook", "P_DIY", "P_Art")
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(train_bal[, predictor_vars],   # All numerical predictor variables
train_bal$Choice,   # Target variable 'Choice'
sizes = c(1:7),     # Number of features to try (adjust as needed)
rfeControl = ctrl,  # Control object for RFE
method = "svmRadial")  # SVM with radial kernel
### Adding SVM Model (with balanced data) ###
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# List of predictor variables
predictor_vars <- c("Amount_purchased", "Frequency", "P_Child", "P_Youth", "P_Cook", "P_DIY", "P_Art")
# Check the structure of train_bal to make sure the variables are correctly formatted
str(train_bal)
# Ensure the 'Choice' column is a factor (or numeric if needed)
train_bal$Choice <- as.factor(train_bal$Choice)
# If needed, convert predictors to numeric
train_bal[predictor_vars] <- lapply(train_bal[predictor_vars], as.numeric)
# Check for any missing values that could cause issues
sum(is.na(train_bal))
# Set up control function for RFE (cross-validation with 10 folds)
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
# List of predictor variables
predictor_vars <- c("Amount_purchased", "Frequency", "P_Child", "P_Youth", "P_Cook", "P_DIY", "P_Art")
# RFE using SVM with radial kernel to select important features
rfe_result <- rfe(
x = train_bal[, predictor_vars],  # Predictor variables (ensure numeric type)
y = train_bal$Choice,             # Target variable 'Choice' (factor)
sizes = c(1:7),                   # Number of features to try
rfeControl = ctrl,                # Control object for RFE
method = "svmRadial"              # SVM with radial kernel
)
pacman::p_load(caret, lattice, tidyverse, gam, logistf, MASS, car, corrplot, gridExtra, ROCR, RCurl, randomForest, readr, readxl, e1071)
##For lit review, write a paper that contains an analysis on bank-related data and compare what analytical techniques they used and worked
#Sample training data set like done in titanic example
##### Data Set ######
url1 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Train.xlsx"
download.file(url1, "BBBC-Train.xlsx", mode = "wb")
BBBC_train <- read_excel("BBBC-Train.xlsx")
url2 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Test.xlsx"
download.file(url2, "BBBC-Test.xlsx", mode = "wb")
BBBC_test <- read_excel("BBBC-Test.xlsx")
#Copy of data set to model
train = BBBC_train
test = BBBC_test
#Turning character variables into factors
fac_vars = c("Choice", "Gender")
train[fac_vars] = lapply(train[fac_vars],as.factor)
test[fac_vars] = lapply(test[fac_vars],as.factor)
##### Balanced? No.#####
plot(train$Choice)
#Correlation Plot
combined = rbind(train, test)
corrplot::corrplot(cor(combined[,c(4:12)]), method = c("number")) #First and last purchased have pretty high correlation
#Normalize data
train = scale(train[,c(4:12)]) #normalize numeric data
train = cbind(BBBC_train[,2:3], train) #combine factor data with normalized data and leaving out the index column ("observation")
train[fac_vars] = lapply(train[fac_vars],as.factor)
str(train)
test = scale(test[,c(4:12)]) #normalize numeric data
test = cbind(BBBC_test[,2:3], test) #combine factor data with normalized data and leaving out the index column ("observation")
test[fac_vars] = lapply(test[fac_vars],as.factor)
str(test)
### BALANCE DATA
train_y = train %>% filter(Choice ==1)
train_n = train %>% filter(Choice ==0)
sample_y = sample_n(train_n, nrow(train_y))
train_bal = rbind(train_y, sample_y)
plot(train_bal$Choice)
test_y = test %>% filter(Choice ==1)
test_n = test %>% filter(Choice ==0)
sample_test_y = sample_n(test_n, nrow(test_y))
test_bal = rbind(test_y, sample_test_y)
plot(test_bal$Choice)
### Linear Model
## LR Model
train$Choice <- as.numeric(as.character(train$Choice))  # it's a factor stored as numbers
train$Gender = as.numeric(as.character(train$Gender))
test$Choice = as.numeric(as.character(test$Choice))
test$Gender = as.numeric(as.character(test$Gender))
m1 = lm(Choice ~., data = train)
vif(m1)
m2 = lm(Choice ~ . -Last_purchase, data = train)
vif(m2)
m3 <- lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY + P_Art,
data = train)
vif(m3)
summary(m3)
m4 = lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train)
summary(m4)
predictions = predict(m4, newdata = test, type = "response")
##when using predict function make sure it's going to new data
#Measures
mse = mean((test$Choice - predictions)^2)
mae = mean(abs(test$Choice - predictions))
me = mean(test$Choice - predictions)
mape =  mean(abs(test$Choice - predictions)/test$Choice)*100
#Multi Collinearity
log.model = glm(Choice ~ . , data = train, family = binomial)
vif(log.model)
log.model2 = glm(Choice ~ . -Last_purchase , data = train, family = binomial) #Remove last_purchased
vif(log.model2)
log.model3 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
vif(log.model3)
#Logistic model
summary(log.model3) #P_Youth not significant
log.model4 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
summary(log.model4)
#Predictions
test$PredProb = predict.glm(log.model4, newdata = test, type = "response")
test$PredSur = ifelse(test$PredProb >= 0.3, 1, 0) # Create new variable converting probabilities to 1s and 0s
# "Confusion Matrix" to get accuracy of the model prediction
caret::confusionMatrix(as.factor(test$PredSur), as.factor(test$Choice) ) #Comparing observed to predicted
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
##### Predicting Profits #####
#Calc profit per book
book_cost = 15*1.45 #each book costs $15 plus 45% overhead
book_profit = 31.95-book_cost #each book is sold for $31.95
#If sending mailings to entire list
total_list = 50000 #list of 50,000 customers to mail ad to
total_cost = 0.65*total_list #cost to mail entire list an ad
#According to case study prompt, 9.03% of mailings resulted in an order
total_profit = 0.0903*total_list*book_profit-total_cost
#If using logistic regression model to selectively send mailings
#predicted (495+138)/(2300)=27.5% would purchase a book with only 138/(495+138) actually buying the book (21.8%)
model_list = total_list*0.275 #Number of customers to send mailings to
model_cost = 0.65*model_list
model_profit = 0.218*model_list*book_profit-model_cost
#Correlation Plot
combined = rbind(train_bal, test_bal)
#Correlation Plot
combined = rbind(train_bal, test)
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + Frequency ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased +  P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child + P_Cook + P_DIY ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child + P_Cook  + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child  + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender  + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
##### Predicting Profits #####
#Calc profit per book
book_cost = 15*1.45 #each book costs $15 plus 45% overhead
book_profit = 31.95-book_cost #each book is sold for $31.95
#If sending mailings to entire list
total_list = 50000 #list of 50,000 customers to mail ad to
total_cost = 0.65*total_list #cost to mail entire list an ad
#According to case study prompt, 9.03% of mailings resulted in an order
total_profit = 0.0903*total_list*book_profit-total_cost
#If using logistic regression model to selectively send mailings
#predicted (495+138)/(2300)=27.5% would purchase a book with only 138/(495+138) actually buying the book (21.8%)
model_list = total_list*0.275 #Number of customers to send mailings to
model_cost = 0.65*model_list
model_profit = 0.218*model_list*book_profit-model_cost
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data and optimal predictors) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.8, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
### Adding SVM Model (with balanced data and optimal predictors) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
