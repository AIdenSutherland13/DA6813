{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e679a37-c5fe-4ab7-85b7-7ebef6cb3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/apps/anaconda3/2024.02-1/lib/python3.11/site-packages/pyarrow/lib.cpython-311-x86_64-linux-gnu.so: undefined symbol: arrow_init_numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/__init__.py:17\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n",
      "File \u001b[0;32m/apps/anaconda3/2024.02-1/lib/python3.11/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: /apps/anaconda3/2024.02-1/lib/python3.11/site-packages/pyarrow/lib.cpython-311-x86_64-linux-gnu.so: undefined symbol: arrow_init_numpy"
     ]
    }
   ],
   "source": [
    "import argparse, re, os\n",
    "from typing import List, Union, Iterable\n",
    "from itertools import zip_longest\n",
    "from compare_mt.rouge.rouge_scorer import RougeScorer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/work/zhy542/tuned-llama-3.1-8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be44b2-cfb2-4877-926e-a82c31e54ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Load Dataset ###\n",
    "####################\n",
    "eval_dataset = load_dataset(\"csv\", data_files=\"./val.csv\", split=\"train\") #, streaming=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b9598-2588-4a11-a6c0-ba5050bb7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Sample from Dataset ###\n",
    "###########################\n",
    "print(next(iter(eval_dataset))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478294d-90a7-4655-8344-3a983146daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = f\"Generate a single headlines for this news article and ensure it has at least one number in the headline: {eval_dataset[\"text\"]}\"\n",
    "#Eval_dataset in format of huggin face dataset (datasets.arrow_dataset.Dataset)\n",
    "print(type(eval_dataset))\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee4748-fa2b-4ef4-bb65-c815eb688d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to list of dictionaries\n",
    "eval_list = eval_dataset.to_dict()\n",
    "eval_list = [dict(zip(eval_list.keys(), values)) for values in zip(*eval_list.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a6017-9ad7-4b81-b7fc-752341206e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a40ae-b620-4e17-8547-1d5dc758e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for item in tqdm(eval_list[:5]): #iterate over first 50 items for now \n",
    "    # Tokenize the input text (the article)\n",
    "    inputs = tokenizer(item[\"text\"], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "   \n",
    "    # Generate the prediction\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5)\n",
    "   \n",
    "    # Decode the generated prediction\n",
    "    predicted_headline = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "   \n",
    "    # Add the prediction to the dictionary\n",
    "    item[\"predictions\"] = predicted_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bca38-f17e-4203-85e6-a70d8f31a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ca870-f335-427d-8d6b-31308c204a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure requirements.txt is downloaded from https://github.com/ArrowHuang/NumHG\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bb7ab-c74d-49fa-9a14-21cbc2d787f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/AIPHES/emnlp19-moverscore.git\n",
    "cd emnlp19-moverscore/\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4291e-2040-4861-bd53-90708e14172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python numhg_eval.py \\\n",
    "--tgt_path=target_path \\\n",
    "--pre_path=predict_result_path \\\n",
    "--num_gt_path=numeral_ground_truth_path \\\n",
    "--num_type_path=numeral_type_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e928b2-8540-4dea-a046-b9c0e0a58d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NumHG Evaluation code\n",
    "#Code above should do this already\n",
    "\n",
    "def cal_num_acc(predict_path, num_gt_path, num_path):\n",
    "    pred_all, gt_all = [], []\n",
    "    pred_copy, gt_copy = [], []\n",
    "    pred_cal, gt_cal = [], []\n",
    "    pattern = re.compile(r'\\d{1,3}(?:,\\d{3})+|\\d+[\\/\\.]{0,1}\\d+|\\d+')\n",
    "\n",
    "    with open(predict_path) as pred, open(num_gt_path) as target, open(num_path) as num_type:\n",
    "        total_num, count_copy, count_cal = 0, 0, 0\n",
    "        for (hyp, gt, num) in zip(pred, target, num_type):\n",
    "            generated_num_list = pattern.findall(hyp)\n",
    "            if(str(num).strip()=='0'): # Copy\n",
    "                count_copy += 1\n",
    "                if(str(gt).strip() in generated_num_list and len(generated_num_list)==1):\n",
    "                    pred_copy.append(1)\n",
    "                    pred_all.append(1)\n",
    "                else:\n",
    "                    pred_copy.append(0)\n",
    "                    pred_all.append(0)\n",
    "                gt_copy.append(1)\n",
    "                gt_all.append(1)\n",
    "            else:\n",
    "                count_cal += 1\n",
    "                if(str(gt).strip() in generated_num_list and len(generated_num_list)==1):\n",
    "                    pred_cal.append(1)\n",
    "                    pred_all.append(1)\n",
    "                else:\n",
    "                    pred_cal.append(0)\n",
    "                    pred_all.append(0)\n",
    "                gt_cal.append(1)\n",
    "                gt_all.append(1)\n",
    "            total_num += 1\n",
    "        print(\"All Accuracy: %.6f, Copy Accuracy: %.6f, Cal Accuracy: %.6f\"%(accuracy_score(gt_all, pred_all), accuracy_score(gt_copy, pred_copy), accuracy_score(gt_cal, pred_cal)))\n",
    "\n",
    "\n",
    "def cal_rouge_score(target_path, predict_path):\n",
    "    rouge1, rouge2, rougeLsum = 0, 0, 0\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "    def process(x):\n",
    "        return sent_tokenize(\" \".join(word_tokenize(x.strip())))\n",
    "\n",
    "    with open(predict_path) as pred, open(target_path) as target:\n",
    "        total_num = 0\n",
    "        for (hyp, ref) in zip(pred, target):\n",
    "            hyp = hyp.strip().strip(\"\\\"\")\n",
    "            ref = ref.strip().strip(\"\\\"\")\n",
    "            hyp = process(hyp)\n",
    "            ref = process(ref)\n",
    "            score = rouge_scorer.score(\"\\n\".join(ref), \"\\n\".join(hyp))\n",
    "            rouge1 += score[\"rouge1\"].fmeasure\n",
    "            rouge2 += score[\"rouge2\"].fmeasure\n",
    "            rougeLsum += score[\"rougeLsum\"].fmeasure\n",
    "            total_num += 1\n",
    "        rouge1 = rouge1 / total_num\n",
    "        rouge2 = rouge2 / total_num\n",
    "        rougeLsum = rougeLsum / total_num\n",
    "        print(\"evaluation rouge1: %.6f, rouge2: %.6f, rougeL: %.6f\"%(rouge1, rouge2, rougeLsum))\n",
    "\n",
    "\n",
    "def cal_mover_score(target_path, predict_path):\n",
    "    from moverscore_v2 import word_mover_score, get_idf_dict\n",
    "    hyp_list, ref_list = [], []\n",
    "    with open(target_path,'r') as g:\n",
    "        for line in g.readlines():\n",
    "            ref_list.append(line.strip())\n",
    "        g.close()\n",
    "\n",
    "    with open(predict_path,'r') as r:\n",
    "        for line in r.readlines():\n",
    "            hyp_list.append(line.strip())\n",
    "        r.close()\n",
    "\n",
    "    idf_dict_hyp = get_idf_dict(hyp_list)\n",
    "    idf_dict_ref = get_idf_dict(ref_list)\n",
    "    mover_score = word_mover_score(ref_list, hyp_list, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=True)\n",
    "    mover = np.mean(mover_score)\n",
    "    print(\"evaluation MoverScore: %.6f\"%(mover))\n",
    "\n",
    "\n",
    "def cal_bert_score(target_path, predict_path):\n",
    "    os.system(\"bert-score -r {} -c {} --lang en --rescale_with_baseline\".format(target_path, predict_path))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print('Calculating Rouge Score......')\n",
    "    cal_rouge_score(args.tgt_path, args.pre_path)\n",
    "\n",
    "    print('\\nCalculating Numeral Accuracy......')\n",
    "    cal_num_acc(args.pre_path, args.num_gt_path, args.num_type_path)\n",
    "\n",
    "    print('\\nCalculating Moverscore......')\n",
    "    cal_mover_score(args.tgt_path, args.pre_path)\n",
    "\n",
    "    print('\\nCalculating BERTScore......')\n",
    "    cal_bert_score(args.tgt_path, args.pre_path)\n",
    "\n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Parameters')\n",
    "    parser.add_argument(\"--tgt_path\", default=\"\", type=str, help=\"target path\")\n",
    "    parser.add_argument(\"--pre_path\", default=\"\", type=str, help=\"predict path\")\n",
    "    parser.add_argument(\"--num_gt_path\", default=\"\", type=str, help=\"numerical ground truth path\")\n",
    "    parser.add_argument(\"--num_type_path\", default=\"\", type=str, help=\"type of each summary, 1:Reasoning, 0:Copy\")\n",
    "    args = parser.parse_args(args=sys.argv[1:] if \"ipykernel\" not in sys.argv[0] else [])\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
