test_n = test %>% filter(Choice ==0)
sample_test_y = sample_n(test_n, nrow(test_y))
test_bal = rbind(test_y, sample_test_y)
plot(test_bal$Choice)
### Linear Model
## LR Model
train$Choice <- as.numeric(as.character(train$Choice))  # it's a factor stored as numbers
train$Gender = as.numeric(as.character(train$Gender))
test$Choice = as.numeric(as.character(test$Choice))
test$Gender = as.numeric(as.character(test$Gender))
m1 = lm(Choice ~., data = train)
vif(m1)
m2 = lm(Choice ~ . -Last_purchase, data = train)
vif(m2)
m3 <- lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY + P_Art,
data = train)
vif(m3)
summary(m3)
m4 = lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train_bal)
summary(m4)
pacman::p_load(caret, lattice, tidyverse, gam, logistf, MASS, car, corrplot, gridExtra, ROCR, RCurl, randomForest, readr, readxl, e1071)
##For lit review, write a paper that contains an analysis on bank-related data and compare what analytical techniques they used and worked
#Sample training data set like done in titanic example
##### Data Set ######
url1 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Train.xlsx"
download.file(url1, "BBBC-Train.xlsx", mode = "wb")
BBBC_train <- read_excel("BBBC-Train.xlsx")
url2 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Test.xlsx"
download.file(url2, "BBBC-Test.xlsx", mode = "wb")
BBBC_test <- read_excel("BBBC-Test.xlsx")
#Copy of data set to model
train = BBBC_train
test = BBBC_test
#Turning character variables into factors
fac_vars = c("Choice", "Gender")
train[fac_vars] = lapply(train[fac_vars],as.factor)
test[fac_vars] = lapply(test[fac_vars],as.factor)
##### Balanced? No.#####
plot(train$Choice)
#Correlation Plot
combined = rbind(train, test)
corrplot::corrplot(cor(combined[,c(4:12)]), method = c("number")) #First and last purchased have pretty high correlation
#Normalize data
train = scale(train[,c(4:12)]) #normalize numeric data
train = cbind(BBBC_train[,2:3], train) #combine factor data with normalized data and leaving out the index column ("observation")
train[fac_vars] = lapply(train[fac_vars],as.factor)
str(train)
test = scale(test[,c(4:12)]) #normalize numeric data
test = cbind(BBBC_test[,2:3], test) #combine factor data with normalized data and leaving out the index column ("observation")
test[fac_vars] = lapply(test[fac_vars],as.factor)
str(test)
### BALANCE DATA
train_y = train %>% filter(Choice ==1)
train_n = train %>% filter(Choice ==0)
sample_y = sample_n(train_n, nrow(train_y))
train_bal = rbind(train_y, sample_y)
plot(train_bal$Choice)
test_y = test %>% filter(Choice ==1)
test_n = test %>% filter(Choice ==0)
sample_test_y = sample_n(test_n, nrow(test_y))
test_bal = rbind(test_y, sample_test_y)
plot(test_bal$Choice)
### Linear Model
## LR Model
train$Choice <- as.numeric(as.character(train$Choice))  # it's a factor stored as numbers
train$Gender = as.numeric(as.character(train$Gender))
test$Choice = as.numeric(as.character(test$Choice))
test$Gender = as.numeric(as.character(test$Gender))
m1 = lm(Choice ~., data = train)
vif(m1)
m2 = lm(Choice ~ . -Last_purchase, data = train)
vif(m2)
m3 <- lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY + P_Art,
data = train)
vif(m3)
summary(m3)
m4 = lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train)
summary(m4)
predictions = predict(m4, newdata = test, type = "response")
##when using predict function make sure it's going to new data
#Measures
mse = mean((test$Choice - predictions)^2)
mae = mean(abs(test$Choice - predictions))
me = mean(test$Choice - predictions)
mape =  mean(abs(test$Choice - predictions)/test$Choice)*100
#Multi Collinearity
log.model = glm(Choice ~ . , data = train, family = binomial)
vif(log.model)
log.model2 = glm(Choice ~ . -Last_purchase , data = train, family = binomial) #Remove last_purchased
vif(log.model2)
log.model3 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
vif(log.model3)
#Logistic model
summary(log.model3) #P_Youth not significant
log.model4 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
summary(log.model4)
#Predictions
test$PredProb = predict.glm(log.model4, newdata = test, type = "response")
test$PredSur = ifelse(test$PredProb >= 0.3, 1, 0) # Create new variable converting probabilities to 1s and 0s
# "Confusion Matrix" to get accuracy of the model prediction
caret::confusionMatrix(as.factor(test$PredSur), as.factor(test$Choice) ) #Comparing observed to predicted
### Adding SVM Model (with balanced data and optimal predictors) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
pacman::p_load(caret, lattice, tidyverse, gam, logistf, MASS, car, corrplot, gridExtra, ROCR, RCurl, randomForest, readr, readxl, e1071)
##For lit review, write a paper that contains an analysis on bank-related data and compare what analytical techniques they used and worked
#Sample training data set like done in titanic example
##### Data Set ######
url1 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Train.xlsx"
download.file(url1, "BBBC-Train.xlsx", mode = "wb")
BBBC_train <- read_excel("BBBC-Train.xlsx")
url2 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Test.xlsx"
download.file(url2, "BBBC-Test.xlsx", mode = "wb")
BBBC_test <- read_excel("BBBC-Test.xlsx")
#Copy of data set to model
train = BBBC_train
test = BBBC_test
#Turning character variables into factors
fac_vars = c("Choice", "Gender")
train[fac_vars] = lapply(train[fac_vars],as.factor)
test[fac_vars] = lapply(test[fac_vars],as.factor)
##### Balanced? No.#####
plot(train$Choice)
#Correlation Plot
combined = rbind(train, test)
corrplot::corrplot(cor(combined[,c(4:12)]), method = c("number")) #First and last purchased have pretty high correlation
#Normalize data
train = scale(train[,c(4:12)]) #normalize numeric data
train = cbind(BBBC_train[,2:3], train) #combine factor data with normalized data and leaving out the index column ("observation")
train[fac_vars] = lapply(train[fac_vars],as.factor)
str(train)
test = scale(test[,c(4:12)]) #normalize numeric data
test = cbind(BBBC_test[,2:3], test) #combine factor data with normalized data and leaving out the index column ("observation")
test[fac_vars] = lapply(test[fac_vars],as.factor)
str(test)
### BALANCE DATA
train_y = train %>% filter(Choice ==1)
train_n = train %>% filter(Choice ==0)
sample_y = sample_n(train_n, nrow(train_y))
train_bal = rbind(train_y, sample_y)
plot(train_bal$Choice)
test_y = test %>% filter(Choice ==1)
test_n = test %>% filter(Choice ==0)
sample_test_y = sample_n(test_n, nrow(test_y))
test_bal = rbind(test_y, sample_test_y)
plot(test_bal$Choice)
### Linear Model
## LR Model
train$Choice <- as.numeric(as.character(train$Choice))  # it's a factor stored as numbers
train$Gender = as.numeric(as.character(train$Gender))
test$Choice = as.numeric(as.character(test$Choice))
test$Gender = as.numeric(as.character(test$Gender))
m1 = lm(Choice ~., data = train)
vif(m1)
m2 = lm(Choice ~ . -Last_purchase, data = train)
vif(m2)
m3 <- lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY + P_Art,
data = train)
vif(m3)
summary(m3)
m4 = lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train)
summary(m4)
predictions = predict(m4, newdata = test, type = "response")
##when using predict function make sure it's going to new data
#Measures
mse = mean((test$Choice - predictions)^2)
mae = mean(abs(test$Choice - predictions))
me = mean(test$Choice - predictions)
mape =  mean(abs(test$Choice - predictions)/test$Choice)*100
#Multi Collinearity
log.model = glm(Choice ~ . , data = train, family = binomial)
vif(log.model)
log.model2 = glm(Choice ~ . -Last_purchase , data = train, family = binomial) #Remove last_purchased
vif(log.model2)
log.model3 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
vif(log.model3)
#Logistic model
summary(log.model3) #P_Youth not significant
log.model4 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
summary(log.model4)
#Predictions
test$PredProb = predict.glm(log.model4, newdata = test, type = "response")
test$PredSur = ifelse(test$PredProb >= 0.3, 1, 0) # Create new variable converting probabilities to 1s and 0s
# "Confusion Matrix" to get accuracy of the model prediction
caret::confusionMatrix(as.factor(test$PredSur), as.factor(test$Choice) ) #Comparing observed to predicted
### Adding SVM Model (with balanced data and optimal predictors) ###
tune_result <- tune(svm,
Choice ~ Gender + Amount_purchased + P_Child ,
data = train_bal,
kernel = "radial",
ranges = list(C = 2^(-5:2), gamma = 2^(-5:2)),
probability = TRUE)
# Best model
svm_best_model <- tune_result$best.model
# Predict with the best model
test_bal$svm_pred <- predict(svm_best_model, test_bal, probability = TRUE)
svm.prob_best <- attr(predict(svm_best_model, test_bal, probability = TRUE), "probabilities")[,2]
# Classify predictions based on a threshold of 0.5
test_bal$svm_class <- ifelse(svm.prob_best >= 0.5, 1, 0)
# Evaluate the tuned SVM model
caret::confusionMatrix(as.factor(test_bal$Choice), as.factor(test_bal$svm_class))
##### Predicting Profits #####
#Calc profit per book
book_cost = 15*1.45 #each book costs $15 plus 45% overhead
book_profit = 31.95-book_cost #each book is sold for $31.95
#If sending mailings to entire list
total_list = 50000 #list of 50,000 customers to mail ad to
total_cost = 0.65*total_list #cost to mail entire list an ad
#According to case study prompt, 9.03% of mailings resulted in an order
total_profit = 0.0903*total_list*book_profit-total_cost
#If using logistic regression model to selectively send mailings
#predicted (495+138)/(2300)=27.5% would purchase a book with only 138/(495+138) actually buying the book (21.8%)
model_list = total_list*0.275 #Number of customers to send mailings to
model_cost = 0.65*model_list
model_profit = 0.218*model_list*book_profit-model_cost
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train_data
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train
test_scaled <- test
# Scale the numeric columns in the training and test sets
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# SVM Model with Grid Search for Hyper parameter Tuning
# Define tuning grid for cost and gamma (for radial kernel)
tune_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100),
sigma = c(0.001, 0.01, 0.1, 1))
# Tuning Radial Basis Function (RBF) kernel SVM
set.seed(123)
svm_tune <- tune(svm, diagnosis ~ ., data = train_scaled,
kernel = "radial",
ranges = list(cost = tune_grid$C, gamma = tune_grid$sigma),
scale = FALSE)  # Don't scale inside the SVM function, we've already done that
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train
test_scaled <- test
# Scale the numeric columns in the training and test sets
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# SVM Model with Grid Search for Hyper parameter Tuning
# Define tuning grid for cost and gamma (for radial kernel)
tune_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100),
sigma = c(0.001, 0.01, 0.1, 1))
# Tuning Radial Basis Function (RBF) kernel SVM
set.seed(123)
svm_tune <- tune(svm, Choice ~ ., data = train_scaled,
kernel = "radial",
ranges = list(cost = tune_grid$C, gamma = tune_grid$sigma),
scale = FALSE)  # Don't scale inside the SVM function, we've already done that
# Best model based on cross-validation
best_model <- svm_tune$best.model
print(svm_tune)
# Test the tuned SVM model
pred_svm <- predict(best_model, newdata = test_scaled)
confusionMatrix(pred_svm, test_scaled$Choice)
test_scaled$Choice <- as.factor(test_scaled$Choice)
pred_svm <- predict(best_model, newdata = test_scaled)
confusionMatrix(pred_svm, test_scaled$Choice)
test_scaled$Choice <- as.factor(test_scaled$Choice)
train_scaled$Choice <- as.factor(train_scaled$Choice)
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# Test the tuned SVM model
pred_svm <- predict(best_model, newdata = test_scaled)
confusionMatrix(pred_svm, test_scaled$Choice)
# Test the tuned SVM model
pred_svm <- factor(pred_svm, levels = levels(test_scaled$Choice))
confusionMatrix(pred_svm, test_scaled$Choice)
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train
test_scaled <- test
# Scale the numeric columns in the training and test sets
test_scaled$Choice <- as.factor(test_scaled$Choice)
train_scaled$Choice <- as.factor(train_scaled$Choice)
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# SVM Model with Grid Search for Hyper parameter Tuning
# Define tuning grid for cost and gamma (for radial kernel)
tune_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100),
sigma = c(0.001, 0.01, 0.1, 1))
# Tuning Radial Basis Function (RBF) kernel SVM
set.seed(123)
svm_tune <- tune(svm, Choice ~ ., data = train_scaled,
kernel = "radial",
ranges = list(cost = tune_grid$C, gamma = tune_grid$sigma),
scale = FALSE)  # Don't scale inside the SVM function, we've already done that
# Best model based on cross-validation
best_model <- svm_tune$best.model
print(svm_tune)
# Test the tuned SVM model
pred_svm <- factor(pred_svm, levels = levels(test_scaled$Choice))
confusionMatrix(pred_svm, test_scaled$Choice)
# Testing Other Kernels
# Linear Kernel
set.seed(123)
svm_linear <- svm(Choice ~ ., data = train_scaled,
kernel = "linear",
cost = best_model$cost,
scale = FALSE)
pred_linear <- predict(svm_linear, newdata = test_scaled)
confusionMatrix(pred_linear, test_scaled$Choice)
# Polynomial Kernel (degree 3)
set.seed(123)
svm_poly <- svm(Choice ~ ., data = train_scaled,
kernel = "polynomial",
cost = best_model$cost,
degree = 3,
scale = FALSE)
pred_poly <- predict(svm_poly, newdata = test_scaled)
confusionMatrix(pred_poly, test_scaled$Choice)
# Compare accuracy across different kernels
linear_acc <- sum(pred_linear == test_scaled$Choice) / nrow(test_scaled)
rbf_acc <- sum(pred_svm == test_scaled$Choice) / nrow(test_scaled)
poly_acc <- sum(pred_poly == test_scaled$Choice) / nrow(test_scaled)
cat("Linear Kernel Accuracy: ", linear_acc, "\n")
cat("RBF Kernel Accuracy: ", rbf_acc, "\n")
cat("Polynomial Kernel Accuracy: ", poly_acc, "\n")
svm_linear <- svm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY
+ P_Art, data = train_scaled,
kernel = "linear",
cost = best_model$cost,
scale = FALSE)
pred_linear <- predict(svm_linear, newdata = test_scaled)
confusionMatrix(pred_linear, test_scaled$Choice)
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train
test_scaled <- test
# Scale the numeric columns in the training and test sets
test_scaled$Choice <- as.factor(test_scaled$Choice)
train_scaled$Choice <- as.factor(train_scaled$Choice)
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# SVM Model with Grid Search for Hyper parameter Tuning
# Define tuning grid for cost and gamma (for radial kernel)
tune_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000),
sigma = c(0.0001, 0.001, 0.01, 0.1, 1))
# Tuning Radial Basis Function (RBF) kernel SVM
set.seed(123)
svm_tune <- tune(svm, Choice ~ ., data = train_scaled,
kernel = "radial",
ranges = list(cost = tune_grid$C, gamma = tune_grid$sigma),
scale = FALSE)  # Don't scale inside the SVM function, we've already done that
pacman::p_load(caret, lattice, tidyverse, gam, logistf, MASS, car, corrplot, gridExtra, ROCR, RCurl, randomForest, readr, readxl, e1071)
##For lit review, write a paper that contains an analysis on bank-related data and compare what analytical techniques they used and worked
#Sample training data set like done in titanic example
##### Data Set ######
url1 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Train.xlsx"
download.file(url1, "BBBC-Train.xlsx", mode = "wb")
BBBC_train <- read_excel("BBBC-Train.xlsx")
url2 <- "https://raw.githubusercontent.com/btj5z2/DA6813/main/BBBC-Test.xlsx"
download.file(url2, "BBBC-Test.xlsx", mode = "wb")
BBBC_test <- read_excel("BBBC-Test.xlsx")
#Copy of data set to model
train = BBBC_train
test = BBBC_test
#Turning character variables into factors
fac_vars = c("Choice", "Gender")
train[fac_vars] = lapply(train[fac_vars],as.factor)
test[fac_vars] = lapply(test[fac_vars],as.factor)
##### Balanced? No.#####
plot(train$Choice)
#Correlation Plot
combined = rbind(train, test)
corrplot::corrplot(cor(combined[,c(4:12)]), method = c("number")) #First and last purchased have pretty high correlation
#Normalize data
train = scale(train[,c(4:12)]) #normalize numeric data
train = cbind(BBBC_train[,2:3], train) #combine factor data with normalized data and leaving out the index column ("observation")
train[fac_vars] = lapply(train[fac_vars],as.factor)
str(train)
test = scale(test[,c(4:12)]) #normalize numeric data
test = cbind(BBBC_test[,2:3], test) #combine factor data with normalized data and leaving out the index column ("observation")
test[fac_vars] = lapply(test[fac_vars],as.factor)
str(test)
### BALANCE DATA
train_y = train %>% filter(Choice ==1)
train_n = train %>% filter(Choice ==0)
sample_y = sample_n(train_n, nrow(train_y))
train_bal = rbind(train_y, sample_y)
plot(train_bal$Choice)
test_y = test %>% filter(Choice ==1)
test_n = test %>% filter(Choice ==0)
sample_test_y = sample_n(test_n, nrow(test_y))
test_bal = rbind(test_y, sample_test_y)
plot(test_bal$Choice)
### Linear Model
## LR Model
train$Choice <- as.numeric(as.character(train$Choice))  # it's a factor stored as numbers
train$Gender = as.numeric(as.character(train$Gender))
test$Choice = as.numeric(as.character(test$Choice))
test$Gender = as.numeric(as.character(test$Gender))
m1 = lm(Choice ~., data = train)
vif(m1)
m2 = lm(Choice ~ . -Last_purchase, data = train)
vif(m2)
m3 <- lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY + P_Art,
data = train)
vif(m3)
summary(m3)
m4 = lm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY + P_Art,
data = train)
summary(m4)
predictions = predict(m4, newdata = test, type = "response")
##when using predict function make sure it's going to new data
#Measures
mse = mean((test$Choice - predictions)^2)
mae = mean(abs(test$Choice - predictions))
me = mean(test$Choice - predictions)
mape =  mean(abs(test$Choice - predictions)/test$Choice)*100
#Multi Collinearity
log.model = glm(Choice ~ . , data = train, family = binomial)
vif(log.model)
log.model2 = glm(Choice ~ . -Last_purchase , data = train, family = binomial) #Remove last_purchased
vif(log.model2)
log.model3 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Youth + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
vif(log.model3)
#Logistic model
summary(log.model3) #P_Youth not significant
log.model4 = glm(Choice ~ Gender + Amount_purchased + Frequency + P_Child + P_Cook + P_DIY
+ P_Art , data = train, family = binomial) #Remove first_purchased
summary(log.model4)
#Predictions
test$PredProb = predict.glm(log.model4, newdata = test, type = "response")
test$PredSur = ifelse(test$PredProb >= 0.3, 1, 0) # Create new variable converting probabilities to 1s and 0s
# "Confusion Matrix" to get accuracy of the model prediction
caret::confusionMatrix(as.factor(test$PredSur), as.factor(test$Choice) ) #Comparing observed to predicted
### Adding SVM Model (with balanced data and optimal predictors) ###
# We'll scale both the training and testing data (except for the target variable)
train_scaled <- train
test_scaled <- test
# Scale the numeric columns in the training and test sets
test_scaled$Choice <- as.factor(test_scaled$Choice)
train_scaled$Choice <- as.factor(train_scaled$Choice)
numeric_cols <- sapply(train_scaled, is.numeric)
train_scaled[numeric_cols] <- scale(train_scaled[numeric_cols])
test_scaled[numeric_cols] <- scale(test_scaled[numeric_cols])
# SVM Model with Grid Search for Hyper parameter Tuning
# Define tuning grid for cost and gamma (for radial kernel)
tune_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000),
sigma = c(0.0001, 0.001, 0.01, 0.1, 1))
# Tuning Radial Basis Function (RBF) kernel SVM
set.seed(123)
svm_tune <- tune(svm, Choice ~ ., data = train_scaled,
kernel = "radial",
ranges = list(cost = tune_grid$C, gamma = tune_grid$sigma),
scale = FALSE)  # Don't scale inside the SVM function, we've already done that
# Best model based on cross-validation
best_model <- svm_tune$best.model
print(svm_tune)
# Test the tuned SVM model
pred_svm <- factor(pred_svm, levels = levels(test_scaled$Choice))
confusionMatrix(pred_svm, test_scaled$Choice)
# Testing Other Kernels
# Linear Kernel
set.seed(123)
svm_linear <- svm(Choice ~ , data = train_scaled,
# Best model based on cross-validation
best_model <- svm_tune$best.model
print(svm_tune)
# Test the tuned SVM model
pred_svm <- predict(best_model, newdata = test_scaled)
confusionMatrix(pred_svm, test_scaled$Choice)
# Testing Other Kernels
# Linear Kernel
set.seed(123)
svm_linear <- svm(Choice ~ ., data = train_scaled,
kernel = "linear",
cost = best_model$cost,
scale = FALSE)
pred_linear <- predict(svm_linear, newdata = test_scaled)
confusionMatrix(pred_linear, test_scaled$Choice)
# Polynomial Kernel (degree 3)
set.seed(123)
svm_poly <- svm(Choice ~ ., data = train_scaled,
kernel = "polynomial",
cost = best_model$cost,
degree = 3,
scale = FALSE)
pred_poly <- predict(svm_poly, newdata = test_scaled)
confusionMatrix(pred_poly, test_scaled$Choice)
# Compare accuracy across different kernels
linear_acc <- sum(pred_linear == test_scaled$Choice) / nrow(test_scaled)
rbf_acc <- sum(pred_svm == test_scaled$Choice) / nrow(test_scaled)
poly_acc <- sum(pred_poly == test_scaled$Choice) / nrow(test_scaled)
cat("Linear Kernel Accuracy: ", linear_acc, "\n")
cat("RBF Kernel Accuracy: ", rbf_acc, "\n")
cat("Polynomial Kernel Accuracy: ", poly_acc, "\n")
